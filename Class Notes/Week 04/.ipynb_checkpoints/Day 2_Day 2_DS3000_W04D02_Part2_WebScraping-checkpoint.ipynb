{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <img src=\"https://github.ccs.neu.edu/caglar/DS3000/blob/master/img/ds3000.png?raw=true\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h1> Week 4 - Day 2 </h1> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h2> Part 2: Web Scraping </h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Outline\n",
    "1. <a href='#1'>Reading Web Resources from URLs</a>\n",
    "2. <a href='#2'>Web Scraping using BeautifulSoup</a>\n",
    "3. <a href='#3'>Scraping Specific Tags from Webpages</a>\n",
    "4. <a href='#4'>Scraping Web Pages by Tags and Attributes</a>\n",
    "5. <a href='#5'>Scraping Child Tags under a Parent Tag</a>\n",
    "6. <a href='#6'>Storing the Scraped Data</a>\n",
    "7. <a href='#7'>Web Crawling</a>\n",
    "8. <a href='#8'>More on Web Scraping</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping\n",
    "* Using programs or scripts to pretend to browse websites, examine the content on those websites, retrieve and extract data from those websites\n",
    "* Also called web spidering, web crawling, web harvesting, or web data extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why Scrape?\n",
    "* Pull your dataset from a website when your data is not readily available\n",
    "* Extract different pieces of information from online resources when working with non-traditional datasets (e.g., social media posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Wyh not Scrape?\n",
    "* It may not be legal\n",
    "* You can't republish copyrighted information\n",
    "* Terms of service violations are not okay\n",
    "* Some websites don't like you scraping their content!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<a id=\"1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Reading Web Resources from URLs\n",
    "* **`urllib`** module allows you to read data from URLs\n",
    "Open the URL url, which can be either a string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as urllib\n",
    "html = urllib.urlopen(\"https://www.khoury.northeastern.edu/role/tenured-and-tenure-track-faculty/\")\n",
    "print(html.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1.1. urlopen() function\n",
    "* Opens a URL, which can be a string or Request object\n",
    "* Returns a file-like object containing the contents of the URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as urllib\n",
    "html = urllib.urlopen(\"https://www.khoury.northeastern.edu/role/tenured-and-tenure-track-faculty/\")\n",
    "print(html.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1.2. read() method\n",
    "* Reads the entire page of the Request object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as urllib\n",
    "html = urllib.urlopen(\"https://www.khoury.northeastern.edu/role/tenured-and-tenure-track-faculty/\")\n",
    "print(html.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<a id=\"2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Web Scraping using BeautifulSoup\n",
    "* A common web scraping library\n",
    "* Helps format and organize the messy web by fixing bad HTML and presenting us with easily-traversible Python objects\n",
    "* Need to install the library before you can use it in Python\n",
    "    * pip install beautifulsoup4 (run this in Anaconda prompt)\n",
    "    * Full documentation available at https://www.crummy.com/software/BeautifulSoup/bs4/doc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#imports the BeautifulSoup object in bs4 library\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import urllib.request as urllib\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urllib.urlopen(\"https://www.khoury.northeastern.edu/role/tenured-and-tenure-track-faculty/\")\n",
    "soup = BeautifulSoup(html.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### soup = BeautifulSoup(htm.read())\n",
    "* Transforms the HTML content into a BeautifulSoup object, called soup\n",
    "* The BeautifulSoup object retains the general structure of a web page:\n",
    "    * `<html></html>`\n",
    "    * `<head></head>`\n",
    "    * `<body></body>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "soup.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.body.main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "soup.body.main.h1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* What if you wanted to retrieve the text contained in `<h1> </h1>`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2.1. get_text() method\n",
    "* **`get_text()`** strips all tags from the webpage and returns a string containing the text inbetween the tags only.\n",
    "* Strips away all the tags and returns a tagless block of text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.body.main.h1.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2.1. get_text() method cont'd\n",
    "* Call .get_text() immediately before you print, store, or manipulate your final data\n",
    "* A lot easier to find what you’re looking for in a BeautifulSoup object than in a block of text\n",
    "* try to preserve the tag structure of a document as long as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "soup.body.main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "soup.body.main.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2.2. get() method\n",
    "* Retrieves an attribute of a tag\n",
    "* **tagName.get(\"attributeName\")**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.body.main.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.body.main.a.get(\"href\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2.3. attrs attribute\n",
    "* Returns a dictionary of the attributes of a tag\n",
    "* **tagName.attrs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.body.main.a.attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **attrs** can also be used to retrieve attributes of a tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "soup.body.main.a.attrs[\"href\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.body.main.a.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2.4. find() method\n",
    "* Allows you to search through an HTML page and find a specific tag\n",
    "* soup_name.find(\"tagName\")\n",
    "* Returns the first occurrence of the tag\n",
    "* Returns None if the tag/attribute does not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(\"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(\"title\").get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<a id=\"3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Scraping Specific Tags from Webpages\n",
    "* **`find_all(tagName)`** returns **a list of all the tags** found within the page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets' extract all the links found on the Khoury Faculty page\n",
    "* Links are placed in `<a>` tags \n",
    "* A typical link in HTML looks like this:\n",
    "    * Backend: `<a href=\"https://www.khoury.northeastern.edu/people/carla-brodley/\"> Carla E. Brodley </a>`\n",
    "    * Frontend: <a href=\"https://www.khoury.northeastern.edu/people/carla-brodley/\"> Carla E. Brodley </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import urllib.request as urllib\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urllib.urlopen(\"https://www.khoury.northeastern.edu/role/tenured-and-tenure-track-faculty/\")\n",
    "soup = BeautifulSoup(html.read())\n",
    "\n",
    "a_tags = soup.find_all(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in a_tags:\n",
    "    if link.get('href').startswith(\"http\"):\n",
    "        print(link.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* What if you justed wanted to scrape the hyperlinks, not email addresses or phone numbers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "for link in a_tags:\n",
    "    href = link.get(\"href\")\n",
    "    if href.startswith(\"https\"):\n",
    "        print(href)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. Scraping Web Pages by Tags and Attributes\n",
    "* Web pages use tags and attributes to style and format pages.\n",
    "* **`find_all()`** method allows you to search through a web page and extract useful information\n",
    "* findAll(tagName, tagAttributes)\n",
    "    * Looks through a tag’s descendants and retrieves all descendants that match your filters\n",
    "    * Returns None if the tag/attribute does not exist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's retrieve the faculty names from the page:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"res/html_tree.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import urllib.request as urllib\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urllib.urlopen(\"https://www.khoury.northeastern.edu/role/tenured-and-tenure-track-faculty/\")\n",
    "soup = BeautifulSoup(html.read())\n",
    "\n",
    "#retrieves all h3 tags with class = \"person-name\"\n",
    "faculty_list = soup.find_all(\"h3\", {\"class\":\"person-name\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faculty_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### find_all() method calls\n",
    "* both retrieve all h3 tags with class = \"person-name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "faculty_list = soup.find_all(\"h3\", {\"class\":\"person-name\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faculty_list = soup.find_all(\"h3\", class_=\"person-name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Now that we have all h3 tags stored in a list, we can extract the text content contained in `<h3></h3>`\n",
    "* Use **get_text()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import urllib.request as urllib\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urllib.urlopen(\"https://www.khoury.northeastern.edu/role/tenured-and-tenure-track-faculty/\")\n",
    "soup = BeautifulSoup(html.read())\n",
    "\n",
    "#retrieves all h3 tags with class = \"person-name\"\n",
    "faculty_list = soup.find_all(\"h3\", {\"class\":\"person-name\"})\n",
    "\n",
    "#retrieves the text contained in each prof's h3 tag\n",
    "for prof in faculty_list:\n",
    "    print(prof.get_text().strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<a id=\"5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "  \n",
    "## 5. Scraping Child Tags under a Parent Tag\n",
    "* Let's create our own record of faculty names, titles, webpage links, and profile picture URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"res/parent_child_tags.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src = \"res/khoury_grid_item.png\" /></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import urllib.request as urllib\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urllib.urlopen(\"https://www.khoury.northeastern.edu/role/tenured-and-tenure-track-faculty/\")\n",
    "soup = BeautifulSoup(html.read())\n",
    "\n",
    "#retrieves all <div class = \"grid-item\"> tags\n",
    "faculty_divs = soup.find_all(\"div\", class_=\"grid-item\")\n",
    "faculty_divs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 5.1. Let's extract faculty names\n",
    "* All names are marked with `<h3 class = \"person-name\">`\n",
    "* Because there is one `<h3>` tag under `<div class = \"grid-item\">` we can use the **find()** method to scrape it\n",
    "* Need to call the **get_text()** on it to get the content\n",
    "* Also need to strip whitespace using **strip()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "faculty_divs = soup.find_all(\"div\", class_=\"grid-item\")\n",
    "\n",
    "for prof in faculty_divs:\n",
    "    fname = prof.find(\"h3\").get_text().strip()\n",
    "    print(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 5.2. Similarly we can get the title of each faculty member\n",
    "* Retrieve the `<p class=\"position\"` tag under `<div class = \"grid-item\">`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faculty_divs = soup.find_all(\"div\", class_=\"grid-item\")\n",
    "\n",
    "for prof in faculty_divs:   \n",
    "    fname = prof.find(\"h3\").get_text().strip()\n",
    "    ftitle= prof.find(\"p\",\"position\").get_text().strip()\n",
    "    \n",
    "    print(fname)\n",
    "    print(ftitle)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 5.3. Can get the link to the faculty member's webpage\n",
    "* Retrieve the **`href`** attribute of the **`<a>`** tag under `<div class = \"grid-item\">`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faculty_divs = soup.find_all(\"div\", class_=\"grid-item\")\n",
    "\n",
    "for prof in faculty_divs:   \n",
    "    fname = prof.find(\"h3\").get_text().strip()\n",
    "    ftitle= prof.find(\"p\",\"position\").get_text().strip()\n",
    "    fpage = prof.find(\"a\").get(\"href\").strip()\n",
    "\n",
    "    print(fname)\n",
    "    print(ftitle)\n",
    "    print(fpage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 5.4. Can get the profile picture too\n",
    "* Retrieve the **`src`** attribute of the **`<img>`** tag under `<div class = \"grid-item\">`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faculty_divs = soup.find_all(\"div\", class_=\"grid-item\")\n",
    "\n",
    "for prof in faculty_divs:   \n",
    "    fname = prof.find(\"h3\").get_text().strip()\n",
    "    ftitle= prof.find(\"p\",\"position\").get_text().strip()\n",
    "    fpage = prof.find(\"a\").get(\"href\").strip()\n",
    "    fpic = prof.find(\"img\").get(\"src\").strip()\n",
    "\n",
    "    print(fname)\n",
    "    print(ftitle)\n",
    "    print(fpage)\n",
    "    print(fpic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<a id=\"6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 6. Storing the Scraped Data\n",
    "* Most of the time, you'll want to store your scraped data in a file.\n",
    "* Consider using DataFrames for tabular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(columns = [\"Name\", \"Title\", \"Link\", \"Picture\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 6.1. Appending Rows to a DataFrame\n",
    "* Use append() method \n",
    "* Pass in a dictionary containing Column names and Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.append({\"Name\": \"Dumbledore\", \"Title\": \"Headmaster\", \"Link\":\"hogwarts.edu\", \n",
    "                \"Picture\":\"hogwarts.edu/dumby.png\"}, ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "faculty_divs = soup.find_all(\"div\", class_=\"grid-item\")\n",
    "\n",
    "for prof in faculty_divs:   \n",
    "    fname = prof.find(\"h3\").get_text().strip()\n",
    "    ftitle= prof.find(\"p\",\"position\").get_text().strip()\n",
    "    fpage = prof.find(\"a\").get(\"href\").strip()\n",
    "    fpic = prof.find(\"img\").get(\"src\").strip()\n",
    "    \n",
    "#appends the fields to their respective columns\n",
    "#note the curly braces \n",
    "    df = df.append({\"Name\":fname, \"Title\":ftitle, \"Link\":fpage, \"Picture\": fpic}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#displays first 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#displays last 5 rows\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 6.2. Writing the DataFrame to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"khoury_faculty.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<a id=\"7\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 7. Web Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "deanURL = \"https://www.khoury.northeastern.edu/people/carla-brodley/\"\n",
    "    \n",
    "dean_page = urllib.urlopen(deanURL)    \n",
    "page_soup = BeautifulSoup(dean_page.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src = \"res/email.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "deanURL = \"https://www.khoury.northeastern.edu/people/carla-brodley/\"\n",
    "\n",
    "#let's open the page\n",
    "dean_page = urllib.urlopen(deanURL)\n",
    "\n",
    "#creates a BeautifulSoup object containing the content of the page\n",
    "page_soup = BeautifulSoup(dean_page.read())\n",
    "\n",
    "#finds the p tage with class = \"contact-email\"\n",
    "email_container = page_soup.find(\"p\", class_=\"contact-email\")\n",
    "\n",
    "#finds the a tag the ithin the contact-email p tag and extracts the text(email address)\n",
    "email = email_container.find(\"a\").get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's do this for everyone on the page!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. Web Crawling\n",
    "* Scrapers traversing multiple pages and even multiple sites\n",
    "* Web crawlers retrieve page contents for a URL, examine that page for another URL, and retrieve that page or some portions of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "\n",
    "#opens the faculty page\n",
    "html = urllib.urlopen(\"https://www.khoury.northeastern.edu/role/tenured-and-tenure-track-faculty/\")\n",
    "#turns the pag into a BeautifulSoup object\n",
    "soup = BeautifulSoup(html.read())\n",
    "\n",
    "#get all the content under <div class = \"grid-item\">\n",
    "faculty_divs = soup.find_all(\"div\", class_=\"grid-item\")\n",
    "\n",
    "#defines an empty list that will contain the email addressed crawled from faculty webpages\n",
    "emails = []\n",
    "\n",
    "for prof in faculty_divs[:3]:\n",
    "    \n",
    "    #gets faculty name for each prof\n",
    "    fname = prof.find(\"h3\").get_text().strip()\n",
    "    #gets the URL to their webpage\n",
    "    fpage = prof.find(\"a\").get(\"href\").strip()\n",
    "    \n",
    "    #opens the URL for each prof\n",
    "    fac_page = urllib.urlopen(fpage)    \n",
    "    #turns the URL for each prof into a BeautifulSoup object\n",
    "    page_soup = BeautifulSoup(fac_page.read())\n",
    "    \n",
    "    #closes the urllib connection so the website won't get mad at us\n",
    "    fac_page.close()\n",
    "    \n",
    "    #on the new page, finds the email container, <p class = \"contact_email\">\n",
    "    email_container = page_soup.find(\"p\", class_=\"contact-email\")\n",
    "    #gets the text for the <a> tage, the email address\n",
    "    email = email_container.find(\"a\").get_text()\n",
    "    \n",
    "    #appends the email to a list and displays it\n",
    "    emails.append(email)\n",
    "    print(email)\n",
    "    \n",
    "    #waits for a random number of seconds(2-5) before moving on to the next prof in the iterable\n",
    "    #done to avoid overwhelming the website and getting blocked as a bot\n",
    "    time.sleep(random.randint(2,6))\n",
    "\n",
    "print(\"\\n\\n\\nDone scraping the addresses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#displays the list of email addresses\n",
    "emails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Let's add these email addresses to our dataframe, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can add a new column\n",
    "df[\"Email\"] = emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"8\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## 8. More on Web Scraping\n",
    "* **BeautifulSoup** Documentation: https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "* **Selenium:** https://selenium-python.readthedocs.io/index.html\n",
    "    * Web automation and scraping; dynamic GET and POST requests; can interact with dynamic web pages, forms, etc.\n",
    "* **Scrapy:** https://scrapy.org/\n",
    "    * Optimized web crawling tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
