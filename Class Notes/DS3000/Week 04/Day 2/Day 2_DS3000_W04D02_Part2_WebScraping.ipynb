{
  "cells": [
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "<center> <img src=\"https://github.ccs.neu.edu/caglar/DS3000/blob/master/img/ds3000.png?raw=true\"> </center>"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<center> <h1> Week 4 - Day 2 </h1> </center>"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<center> <h2> Part 2: Web Scraping </h2></center>"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "skip"
        }
      },
      "cell_type": "markdown",
      "source": "## Outline\n1. <a href='#1'>Reading Web Resources from URLs</a>\n2. <a href='#2'>Web Scraping using BeautifulSoup</a>\n3. <a href='#3'>Scraping Specific Tags from Webpages</a>\n4. <a href='#4'>Scraping Web Pages by Tags and Attributes</a>\n5. <a href='#5'>Scraping Child Tags under a Parent Tag</a>\n6. <a href='#6'>Storing the Scraped Data</a>\n7. <a href='#7'>Web Crawling</a>\n8. <a href='#8'>More on Web Scraping</a>\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Web Scraping\n* Using programs or scripts to pretend to browse websites, examine the content on those websites, retrieve and extract data from those websites\n* Also called web spidering, web crawling, web harvesting, or web data extraction"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "## Why Scrape?\n* Pull your dataset from a website when your data is not readily available\n* Extract different pieces of information from online resources when working with non-traditional datasets (e.g., social media posts)"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "## Wyh not Scrape?\n* It may not be legal\n* You can't republish copyrighted information\n* Terms of service violations are not okay\n* Some websites don't like you scraping their content!"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "skip"
        }
      },
      "cell_type": "markdown",
      "source": "<a id=\"1\"></a>"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "## 1. Reading Web Resources from URLs\n* **`urllib`** module allows you to read data from URLs\nOpen the URL url, which can be either a string "
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "import urllib.request as urllib\nhtml = urllib.urlopen(\"https://www.khoury.northeastern.edu/role/tenured-and-tenure-track-faculty/\")\nprint(html.read())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "### 1.1. urlopen() function\n* Opens a URL, which can be a string or Request object\n* Returns a file-like object containing the contents of the URL"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "import urllib.request as urllib\nhtml = urllib.urlopen(\"https://www.khoury.northeastern.edu/role/tenured-and-tenure-track-faculty/\")\nprint(html.read())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "### 1.2. read() method\n* Reads the entire page of the Request object"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "import urllib.request as urllib\nhtml = urllib.urlopen(\"https://www.khoury.northeastern.edu/role/tenured-and-tenure-track-faculty/\")\nprint(html.read())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "skip"
        }
      },
      "cell_type": "markdown",
      "source": "<a id=\"2\"></a>"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "## 2. Web Scraping using BeautifulSoup\n* A common web scraping library\n* Helps format and organize the messy web by fixing bad HTML and presenting us with easily-traversible Python objects\n* Need to install the library before you can use it in Python\n    * pip install beautifulsoup4 (run this in Anaconda prompt)\n    * Full documentation available at https://www.crummy.com/software/BeautifulSoup/bs4/doc/"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "-"
        },
        "trusted": false
      },
      "cell_type": "code",
      "source": "#imports the BeautifulSoup object in bs4 library\nfrom bs4 import BeautifulSoup",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        },
        "trusted": false
      },
      "cell_type": "code",
      "source": "import urllib.request as urllib\nfrom bs4 import BeautifulSoup\n\nhtml = urllib.urlopen(\"https://www.khoury.northeastern.edu/role/tenured-and-tenure-track-faculty/\")\nsoup = BeautifulSoup(html.read())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "-"
        }
      },
      "cell_type": "markdown",
      "source": "### soup = BeautifulSoup(htm.read())\n* Transforms the HTML content into a BeautifulSoup object, called soup\n* The BeautifulSoup object retains the general structure of a web page:\n    * `<html></html>`\n    * `<head></head>`\n    * `<body></body>`"
    },
    {
      "metadata": {
        "trusted": false,
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "code",
      "source": "soup.html",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "soup.head",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "soup.body",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "soup.body.main",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        },
        "trusted": false
      },
      "cell_type": "code",
      "source": "soup.body.main.h1",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "cell_type": "markdown",
      "source": "* What if you wanted to retrieve the text contained in `<h1> </h1>`?"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "### 2.1. get_text() method\n* **`get_text()`** strips all tags from the webpage and returns a string containing the text inbetween the tags only.\n* Strips away all the tags and returns a tagless block of text\n"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "soup.body.main.h1.get_text()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "### 2.1. get_text() method cont'd\n* Call .get_text() immediately before you print, store, or manipulate your final data\n* A lot easier to find what youâ€™re looking for in a BeautifulSoup object than in a block of text\n* try to preserve the tag structure of a document as long as possible"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        },
        "trusted": false
      },
      "cell_type": "code",
      "source": "soup.body.main",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        },
        "trusted": false
      },
      "cell_type": "code",
      "source": "soup.body.main.get_text()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "### 2.2. get() method\n* Retrieves an attribute of a tag\n* **tagName.get(\"attributeName\")**"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "soup.body.main.a",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "soup.body.main.a.get(\"href\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "### 2.3. attrs attribute\n* Returns a dictionary of the attributes of a tag\n* **tagName.attrs**"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "soup.body.main.a.attrs",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "cell_type": "markdown",
      "source": "* **attrs** can also be used to retrieve attributes of a tag:"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        },
        "trusted": false
      },
      "cell_type": "code",
      "source": "soup.body.main.a.attrs[\"href\"]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "soup.body.main.a.get_text()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "### 2.4. find() method\n* Allows you to search through an HTML page and find a specific tag\n* soup_name.find(\"tagName\")\n* Returns the first occurrence of the tag\n* Returns None if the tag/attribute does not exist"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "soup.find(\"title\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "soup.find(\"title\").get_text()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "skip"
        }
      },
      "cell_type": "markdown",
      "source": "<a id=\"3\"></a>"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "## 3. Scraping Specific Tags from Webpages\n* **`find_all(tagName)`** returns **a list of all the tags** found within the page."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Lets' extract all the links found on the Khoury Faculty page\n* Links are placed in `<a>` tags \n* A typical link in HTML looks like this:\n    * Backend: `<a href=\"https://www.khoury.northeastern.edu/people/carla-brodley/\"> Carla E. Brodley </a>`\n    * Frontend: <a href=\"https://www.khoury.northeastern.edu/people/carla-brodley/\"> Carla E. Brodley </a>"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        },
        "trusted": false
      },
      "cell_type": "code",
      "source": "import urllib.request as urllib\nfrom bs4 import BeautifulSoup\n\nhtml = urllib.urlopen(\"https://www.khoury.northeastern.edu/role/tenured-and-tenure-track-faculty/\")\nsoup = BeautifulSoup(html.read())\n\na_tags = soup.find_all(\"a\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "a_tags",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "for link in a_tags:\n    if link.get('href').startswith(\"http\"):\n        print(link.get('href'))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "* What if you justed wanted to scrape the hyperlinks, not email addresses or phone numbers?"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        },
        "trusted": false
      },
      "cell_type": "code",
      "source": "for link in a_tags:\n    href = link.get(\"href\")\n    if href.startswith(\"https\"):\n        print(href)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<a id=\"4\"></a>"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "## 4. Scraping Web Pages by Tags and Attributes\n* Web pages use tags and attributes to style and format pages.\n* **`find_all()`** method allows you to search through a web page and extract useful information\n* findAll(tagName, tagAttributes)\n    * Looks through a tagâ€™s descendants and retrieves all descendants that match your filters\n    * Returns None if the tag/attribute does not exist\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "* Let's retrieve the faculty names from the page:"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "<img src=\"res/html_tree.png\" />"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        },
        "trusted": false
      },
      "cell_type": "code",
      "source": "import urllib.request as urllib\nfrom bs4 import BeautifulSoup\n\nhtml = urllib.urlopen(\"https://www.khoury.northeastern.edu/role/tenured-and-tenure-track-faculty/\")\nsoup = BeautifulSoup(html.read())\n\n#retrieves all h3 tags with class = \"person-name\"\nfaculty_list = soup.find_all(\"h3\", {\"class\":\"person-name\"})",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "faculty_list",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "### find_all() method calls\n* both retrieve all h3 tags with class = \"person-name\""
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "-"
        },
        "trusted": false
      },
      "cell_type": "code",
      "source": "faculty_list = soup.find_all(\"h3\", {\"class\":\"person-name\"})",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "faculty_list = soup.find_all(\"h3\", class_=\"person-name\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "* Now that we have all h3 tags stored in a list, we can extract the text content contained in `<h3></h3>`\n* Use **get_text()**"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "-"
        },
        "trusted": false
      },
      "cell_type": "code",
      "source": "import urllib.request as urllib\nfrom bs4 import BeautifulSoup\n\nhtml = urllib.urlopen(\"https://www.khoury.northeastern.edu/role/tenured-and-tenure-track-faculty/\")\nsoup = BeautifulSoup(html.read())\n\n#retrieves all h3 tags with class = \"person-name\"\nfaculty_list = soup.find_all(\"h3\", {\"class\":\"person-name\"})\n\n#retrieves the text contained in each prof's h3 tag\nfor prof in faculty_list:\n    print(prof.get_text().strip())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "skip"
        }
      },
      "cell_type": "markdown",
      "source": "<a id=\"5\"></a>"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "  \n## 5. Scraping Child Tags under a Parent Tag\n* Let's create our own record of faculty names, titles, webpage links, and profile picture URLs"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<img src = \"res/parent_child_tags.png\" />"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "<center><img src = \"res/khoury_grid_item.png\" /></center>"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        },
        "trusted": false
      },
      "cell_type": "code",
      "source": "import urllib.request as urllib\nfrom bs4 import BeautifulSoup\n\nhtml = urllib.urlopen(\"https://www.khoury.northeastern.edu/role/tenured-and-tenure-track-faculty/\")\nsoup = BeautifulSoup(html.read())\n\n#retrieves all <div class = \"grid-item\"> tags\nfaculty_divs = soup.find_all(\"div\", class_=\"grid-item\")\nfaculty_divs",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "### 5.1. Let's extract faculty names\n* All names are marked with `<h3 class = \"person-name\">`\n* Because there is one `<h3>` tag under `<div class = \"grid-item\">` we can use the **find()** method to scrape it\n* Need to call the **get_text()** on it to get the content\n* Also need to strip whitespace using **strip()**"
    },
    {
      "metadata": {
        "scrolled": true,
        "slideshow": {
          "slide_type": "-"
        },
        "trusted": false
      },
      "cell_type": "code",
      "source": "faculty_divs = soup.find_all(\"div\", class_=\"grid-item\")\n\nfor prof in faculty_divs:\n    fname = prof.find(\"h3\").get_text().strip()\n    print(fname)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "### 5.2. Similarly we can get the title of each faculty member\n* Retrieve the `<p class=\"position\"` tag under `<div class = \"grid-item\">`"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "faculty_divs = soup.find_all(\"div\", class_=\"grid-item\")\n\nfor prof in faculty_divs:   \n    fname = prof.find(\"h3\").get_text().strip()\n    ftitle= prof.find(\"p\",\"position\").get_text().strip()\n    \n    print(fname)\n    print(ftitle)  \n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "### 5.3. Can get the link to the faculty member's webpage\n* Retrieve the **`href`** attribute of the **`<a>`** tag under `<div class = \"grid-item\">`"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "faculty_divs = soup.find_all(\"div\", class_=\"grid-item\")\n\nfor prof in faculty_divs:   \n    fname = prof.find(\"h3\").get_text().strip()\n    ftitle= prof.find(\"p\",\"position\").get_text().strip()\n    fpage = prof.find(\"a\").get(\"href\").strip()\n\n    print(fname)\n    print(ftitle)\n    print(fpage)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "### 5.4. Can get the profile picture too\n* Retrieve the **`src`** attribute of the **`<img>`** tag under `<div class = \"grid-item\">`"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "faculty_divs = soup.find_all(\"div\", class_=\"grid-item\")\n\nfor prof in faculty_divs:   \n    fname = prof.find(\"h3\").get_text().strip()\n    ftitle= prof.find(\"p\",\"position\").get_text().strip()\n    fpage = prof.find(\"a\").get(\"href\").strip()\n    fpic = prof.find(\"img\").get(\"src\").strip()\n\n    print(fname)\n    print(ftitle)\n    print(fpage)\n    print(fpic)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "skip"
        }
      },
      "cell_type": "markdown",
      "source": "<a id=\"6\"></a>"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "## 6. Storing the Scraped Data\n* Most of the time, you'll want to store your scraped data in a file.\n* Consider using DataFrames for tabular data."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "import pandas as pd\n\ndf = pd.DataFrame(columns = [\"Name\", \"Title\", \"Link\", \"Picture\"])\ndf",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "### 6.1. Appending Rows to a DataFrame\n* Use append() method \n* Pass in a dictionary containing Column names and Values"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df = df.append({\"Name\": \"Dumbledore\", \"Title\": \"Headmaster\", \"Link\":\"hogwarts.edu\", \n                \"Picture\":\"hogwarts.edu/dumby.png\"}, ignore_index=True)\ndf",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df = df.drop(0)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        },
        "trusted": false
      },
      "cell_type": "code",
      "source": "faculty_divs = soup.find_all(\"div\", class_=\"grid-item\")\n\nfor prof in faculty_divs:   \n    fname = prof.find(\"h3\").get_text().strip()\n    ftitle= prof.find(\"p\",\"position\").get_text().strip()\n    fpage = prof.find(\"a\").get(\"href\").strip()\n    fpic = prof.find(\"img\").get(\"src\").strip()\n    \n#appends the fields to their respective columns\n#note the curly braces \n    df = df.append({\"Name\":fname, \"Title\":ftitle, \"Link\":fpage, \"Picture\": fpic}, ignore_index=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        },
        "trusted": false
      },
      "cell_type": "code",
      "source": "#displays first 5 rows\ndf.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "#displays last 5 rows\ndf.tail()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "### 6.2. Writing the DataFrame to CSV"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df.to_csv(\"khoury_faculty.csv\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "skip"
        }
      },
      "cell_type": "markdown",
      "source": "<a id=\"7\"></a>"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "## 7. Web Crawling"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "-"
        },
        "trusted": false
      },
      "cell_type": "code",
      "source": "deanURL = \"https://www.khoury.northeastern.edu/people/carla-brodley/\"\n    \ndean_page = urllib.urlopen(deanURL)    \npage_soup = BeautifulSoup(dean_page.read())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "page_soup",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "<img src = \"res/email.png\" />"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "-"
        },
        "trusted": false
      },
      "cell_type": "code",
      "source": "deanURL = \"https://www.khoury.northeastern.edu/people/carla-brodley/\"\n\n#let's open the page\ndean_page = urllib.urlopen(deanURL)\n\n#creates a BeautifulSoup object containing the content of the page\npage_soup = BeautifulSoup(dean_page.read())\n\n#finds the p tage with class = \"contact-email\"\nemail_container = page_soup.find(\"p\", class_=\"contact-email\")\n\n#finds the a tag the ithin the contact-email p tag and extracts the text(email address)\nemail = email_container.find(\"a\").get_text()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "email",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Let's do this for everyone on the page!"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 7.1. Web Crawling\n* Scrapers traversing multiple pages and even multiple sites\n* Web crawlers retrieve page contents for a URL, examine that page for another URL, and retrieve that page or some portions of it"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "import urllib.request as urllib\nfrom bs4 import BeautifulSoup\nimport time\nimport random\n\n#opens the faculty page\nhtml = urllib.urlopen(\"https://www.khoury.northeastern.edu/role/tenured-and-tenure-track-faculty/\")\n#turns the pag into a BeautifulSoup object\nsoup = BeautifulSoup(html.read())\n\n#get all the content under <div class = \"grid-item\">\nfaculty_divs = soup.find_all(\"div\", class_=\"grid-item\")\n\n#defines an empty list that will contain the email addressed crawled from faculty webpages\nemails = []\n\nfor prof in faculty_divs[:3]:\n    \n    #gets faculty name for each prof\n    fname = prof.find(\"h3\").get_text().strip()\n    #gets the URL to their webpage\n    fpage = prof.find(\"a\").get(\"href\").strip()\n    \n    #opens the URL for each prof\n    fac_page = urllib.urlopen(fpage)    \n    #turns the URL for each prof into a BeautifulSoup object\n    page_soup = BeautifulSoup(fac_page.read())\n    \n    #closes the urllib connection so the website won't get mad at us\n    fac_page.close()\n    \n    #on the new page, finds the email container, <p class = \"contact_email\">\n    email_container = page_soup.find(\"p\", class_=\"contact-email\")\n    #gets the text for the <a> tage, the email address\n    email = email_container.find(\"a\").get_text()\n    \n    #appends the email to a list and displays it\n    emails.append(email)\n    print(email)\n    \n    #waits for a random number of seconds(2-5) before moving on to the next prof in the iterable\n    #done to avoid overwhelming the website and getting blocked as a bot\n    time.sleep(random.randint(2,6))\n\nprint(\"\\n\\n\\nDone scraping the addresses\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "#displays the list of email addresses\nemails",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "#### Let's add these email addresses to our dataframe, df"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "len(emails)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "#we can add a new column\ndf[\"Email\"] = emails",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<a id=\"8\"></a>"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "cell_type": "markdown",
      "source": "## 8. More on Web Scraping\n* **BeautifulSoup** Documentation: https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n* **Selenium:** https://selenium-python.readthedocs.io/index.html\n    * Web automation and scraping; dynamic GET and POST requests; can interact with dynamic web pages, forms, etc.\n* **Scrapy:** https://scrapy.org/\n    * Optimized web crawling tasks"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        },
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "file_extension": ".py",
      "version": "3.5.4",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}