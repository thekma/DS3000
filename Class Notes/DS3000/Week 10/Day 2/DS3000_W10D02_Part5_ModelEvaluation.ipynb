{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <img src=\"res/ds3000.png\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h1> Week 10 - Day 2 </h1> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h2> Part 5: Model Evaluation </h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Outline\n",
    "1. <a href='#1'>Metrics for Measuring Model Accuracy</a>\n",
    "2. <a href='#2'>Prediction Accuracy</a>\n",
    "3. <a href='#3'>Confusion Matrix</a>\n",
    "4. <a href='#4'>Classification Report</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<a id=\"1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<a id=\"1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Metrics for Measuring Model Accuracy\n",
    "* Prediction accuracy\n",
    "* Confusion matrix\n",
    "* Classification report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## 2. Prediction Accuracy\n",
    "Estimator Method `score`\n",
    "* Returns an **indication of how well the estimator performs** on **test data** \n",
    "* For **classification estimators**, returns the **prediction accuracy** for the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "#load the digits dataset\n",
    "digits = load_digits()\n",
    "\n",
    "df = pd.DataFrame(digits.data)\n",
    "df[\"target\"] = digits.target\n",
    "\n",
    "features = df.drop(\"target\", axis = 1)\n",
    "target = df[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, random_state=3000)\n",
    "\n",
    "#select a classifier\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "#create the model by fitting the training data\n",
    "knn.fit(X=X_train, y=y_train)\n",
    "\n",
    "#make predictions on the test set\n",
    "predicted = knn.predict(X=X_test)\n",
    "\n",
    "expected = y_test\n",
    "\n",
    "#prediction accuracy\n",
    "accuracy = knn.score(X_test, y_test)\n",
    "print(\"Prediction accuracy on the test data:\", format(accuracy*100, \".2f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Confusion Matrix\n",
    "* Shows correct and incorrect predicted values (the **hits** and **misses**) for a given class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = confusion_matrix(y_true=expected, y_pred=predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "confusion_df = pd.DataFrame(confusion, index=range(10), columns=range(10))\n",
    "confusion_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "* **Correct predictions** shown on **principal diagonal** from top-left to bottom-right\n",
    "* **Nonzero values** not on **principal diagonal** indicate **incorrect predictions** \n",
    "* Each **row** represents **one distinct class** (0–9) \n",
    "* **Columns** specify how many **test samples** were classified into classes 0–9 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* **Row 0** shows digit class **`0`**&mdash;**all 0s were predicted correctly**\n",
    ">`[45,  0,  0,  0,  0,  0,  0,  0,  0,  0]`\n",
    "* **Row 8** shows digit class **`8`**&mdash;**two 8s were predicted incorrectly**\n",
    ">`[0,  1,  0,  1,  0,  0,  0,  0, 39,  0]`\n",
    "\n",
    "    * **Correctly predicted 95.12%** (39 of 41) of `8`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "39/41"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. Classification Report\n",
    "* classification_report() method produces a report of all important classification statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "class_report = classification_report(y_true=expected, y_pred=predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Precision**: the ability of the classifier not to label a non-8 digit as 8 (look through each column)\n",
    "    * What fraction of predictions of digit 8 are correct? \n",
    "* **Recall**: the ability of the classifier to find all 8s. (look through each row)\n",
    "    *  What fraction of all digit 8s in the testing set does the classifier correctly identify as digit 8? \n",
    "* **F-1 score**: a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0.\n",
    "    * Want to maximize this. The closer to 1 the better\n",
    "* **Support**: the number of occurrences of each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center><img src = \"res/digits0-9.png\" width=400/></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
